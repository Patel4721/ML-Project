{"name":"Ml-project","tagline":"Machine Learning on Coursera","body":"<div id=\"header\">\r\n<h1 class=\"title\">Machine Learning Course Project</h1>\r\n<h4 class=\"author\"><em>Shailesh Patel</em></h4>\r\n<h4 class=\"date\"><em>March 22, 2015</em></h4>\r\n</div>\r\n\r\n\r\n<div id=\"background\" class=\"section level1\">\r\n<h1>Background</h1>\r\n<p>This project is related to Human Activity Research (HAR) and the use of personal activity data captured via devices such as Jawbone Up, Nike FuelBand, and Fitbit. The dataset contains data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions. We are provided a data set from:</p>\r\n<p><a href=\"http://groupware.les.inf.puc-rio.br/har\">http://groupware.les.inf.puc-rio.br/har</a></p>\r\n<p>Our goal is to use the data set to predict the quality of the exercise the user conducted. The quality outcome is defined as classes as follows:</p>\r\n<p>A - exactly according to the specification B - throwing the elbows to the front C - lifting the dumbbell only halfway D - lowering the dumbbell only halfway E - throwing the hips to the front</p>\r\n</div>\r\n<div id=\"reproducibility\" class=\"section level1\">\r\n<h1>Reproducibility</h1>\r\n<p>We will first load the necessary libraries and set the seed.</p>\r\n<pre class=\"r\"><code># Install and load the libraries we will need - only if needed\r\nlist.of.packages &lt;- c(&quot;caret&quot;, \r\n                      &quot;kernlab&quot;, \r\n                      &quot;randomForest&quot;, \r\n                      &quot;Hmisc&quot;, \r\n                      &quot;abind&quot;, \r\n                      &quot;arm&quot;, \r\n                      &quot;rpart&quot;, \r\n                      &quot;doParallel&quot;)\r\nnew.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])]\r\nif(length(new.packages)) install.packages(new.packages)\r\n\r\nlibrary(caret)</code></pre>\r\n<pre><code>## Loading required package: lattice\r\n## Loading required package: ggplot2</code></pre>\r\n<pre class=\"r\"><code>library(kernlab)\r\nlibrary(randomForest)</code></pre>\r\n<pre><code>## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.</code></pre>\r\n<pre class=\"r\"><code>library(Hmisc)</code></pre>\r\n<pre><code>## Loading required package: grid\r\n## Loading required package: survival\r\n## Loading required package: splines\r\n## \r\n## Attaching package: 'survival'\r\n## \r\n## The following object is masked from 'package:caret':\r\n## \r\n##     cluster\r\n## \r\n## Loading required package: Formula\r\n## \r\n## Attaching package: 'Hmisc'\r\n## \r\n## The following object is masked from 'package:randomForest':\r\n## \r\n##     combine\r\n## \r\n## The following objects are masked from 'package:base':\r\n## \r\n##     format.pval, round.POSIXt, trunc.POSIXt, units</code></pre>\r\n<pre class=\"r\"><code>library(abind)\r\nlibrary(arm)</code></pre>\r\n<pre><code>## Loading required package: MASS\r\n## Loading required package: Matrix\r\n## Loading required package: lme4\r\n## Loading required package: Rcpp\r\n## \r\n## arm (Version 1.7-07, built: 2014-8-27)\r\n## \r\n## Working directory is /Users/shaileshpatel/Library/Mobile Documents/com~apple~CloudDocs/Coursera/Machine Learning/Course Project/ML-Project</code></pre>\r\n<pre class=\"r\"><code>library(rpart)\r\nlibrary(doParallel)</code></pre>\r\n<pre><code>## Loading required package: foreach\r\n## Loading required package: iterators\r\n## Loading required package: parallel</code></pre>\r\n<pre class=\"r\"><code>set.seed(8666)</code></pre>\r\n</div>\r\n<div id=\"data\" class=\"section level1\">\r\n<h1>Data</h1>\r\n<div id=\"load-libraries-and-download-data\" class=\"section level2\">\r\n<h2>Load Libraries and Download Data</h2>\r\n<p>The training data for this project wre loaded from the following URLs:</p>\r\n<p>Training data: <a href=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>\r\n<p>Test data: <a href=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>\r\n<p>Download the data for training and for final testing. We call the testing data “final testing” data to avoid confusion with the test set we’ll create to validate our model. Final testing is used for the submission of the results of this work.</p>\r\n<pre class=\"r\"><code>trainingcsv &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;\r\ntestingcsv &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;\r\n\r\ndownload.file(trainingcsv, &quot;pml-training.csv&quot;, method=&quot;curl&quot;)\r\ndownload.file(testingcsv, &quot;pml-testing.csv&quot;, method=&quot;curl&quot;)\r\n\r\ndate() # Log the date and time when the file is downloaded</code></pre>\r\n<pre><code>## [1] &quot;Mon Mar 16 23:35:47 2015&quot;</code></pre>\r\n<pre class=\"r\"><code>trainingData &lt;- read.csv(&quot;pml-training.csv&quot;)\r\nfinalTestingData &lt;- read.csv(&quot;pml-testing.csv&quot;)</code></pre>\r\n</div>\r\n<div id=\"cleaning-data\" class=\"section level2\">\r\n<h2>Cleaning Data</h2>\r\n<p>Make sure all the column names between the training and final testing data sets are identical. We know the final testing data does not contain the “classe” column.</p>\r\n<pre class=\"r\"><code>all.equal(colnames(finalTestingData)[1:length(colnames(finalTestingData))-1], colnames(trainingData)[1:length(colnames(trainingData))-1])</code></pre>\r\n<pre><code>## [1] TRUE</code></pre>\r\n<p>To clean the data, we are first removing the near zero variance predictors. Many variables have a high degree of correlation.</p>\r\n<pre class=\"r\"><code>nzvValues &lt;- nearZeroVar(trainingData)\r\ntrainingData &lt;- trainingData[, -nzvValues]\r\ndim(trainingData)</code></pre>\r\n<pre><code>## [1] 19622   100</code></pre>\r\n<p>We then remove the first five predictors/columns of the training data id, user names, and time stamps are not very useful with models.</p>\r\n<pre class=\"r\"><code>trainingData &lt;- trainingData[, -(1:5)]\r\ndim(trainingData)</code></pre>\r\n<pre><code>## [1] 19622    95</code></pre>\r\n<p>Lastly, we remove columns with too many NA’s. We want to reduce predictors with more than 60% missing data.</p>\r\n<pre class=\"r\"><code>remove &lt;- sapply(colnames(trainingData), function(x) if(sum(is.na(trainingData[, x])) \r\n                                                        &gt; 0.60*nrow(trainingData))\r\n  {return(TRUE)\r\n} else {\r\nreturn(FALSE)})\r\ntrainingData &lt;- trainingData[, !remove]\r\ndim(trainingData)</code></pre>\r\n<pre><code>## [1] 19622    54</code></pre>\r\n<p>After cleaning the data the following predictors are included in the models.</p>\r\n<pre class=\"r\"><code>names(trainingData)</code></pre>\r\n<pre><code>##  [1] &quot;num_window&quot;           &quot;roll_belt&quot;            &quot;pitch_belt&quot;          \r\n##  [4] &quot;yaw_belt&quot;             &quot;total_accel_belt&quot;     &quot;gyros_belt_x&quot;        \r\n##  [7] &quot;gyros_belt_y&quot;         &quot;gyros_belt_z&quot;         &quot;accel_belt_x&quot;        \r\n## [10] &quot;accel_belt_y&quot;         &quot;accel_belt_z&quot;         &quot;magnet_belt_x&quot;       \r\n## [13] &quot;magnet_belt_y&quot;        &quot;magnet_belt_z&quot;        &quot;roll_arm&quot;            \r\n## [16] &quot;pitch_arm&quot;            &quot;yaw_arm&quot;              &quot;total_accel_arm&quot;     \r\n## [19] &quot;gyros_arm_x&quot;          &quot;gyros_arm_y&quot;          &quot;gyros_arm_z&quot;         \r\n## [22] &quot;accel_arm_x&quot;          &quot;accel_arm_y&quot;          &quot;accel_arm_z&quot;         \r\n## [25] &quot;magnet_arm_x&quot;         &quot;magnet_arm_y&quot;         &quot;magnet_arm_z&quot;        \r\n## [28] &quot;roll_dumbbell&quot;        &quot;pitch_dumbbell&quot;       &quot;yaw_dumbbell&quot;        \r\n## [31] &quot;total_accel_dumbbell&quot; &quot;gyros_dumbbell_x&quot;     &quot;gyros_dumbbell_y&quot;    \r\n## [34] &quot;gyros_dumbbell_z&quot;     &quot;accel_dumbbell_x&quot;     &quot;accel_dumbbell_y&quot;    \r\n## [37] &quot;accel_dumbbell_z&quot;     &quot;magnet_dumbbell_x&quot;    &quot;magnet_dumbbell_y&quot;   \r\n## [40] &quot;magnet_dumbbell_z&quot;    &quot;roll_forearm&quot;         &quot;pitch_forearm&quot;       \r\n## [43] &quot;yaw_forearm&quot;          &quot;total_accel_forearm&quot;  &quot;gyros_forearm_x&quot;     \r\n## [46] &quot;gyros_forearm_y&quot;      &quot;gyros_forearm_z&quot;      &quot;accel_forearm_x&quot;     \r\n## [49] &quot;accel_forearm_y&quot;      &quot;accel_forearm_z&quot;      &quot;magnet_forearm_x&quot;    \r\n## [52] &quot;magnet_forearm_y&quot;     &quot;magnet_forearm_z&quot;     &quot;classe&quot;</code></pre>\r\n</div>\r\n<div id=\"partitioning-data\" class=\"section level2\">\r\n<h2>Partitioning Data</h2>\r\n<p>We’ll first partition the data so that 60% of the data will be used for training and 40% will be used for testing the models.</p>\r\n<pre class=\"r\"><code>inTrain &lt;- createDataPartition(y=trainingData$classe, p=0.6, list=FALSE)\r\nmodelTraining &lt;- trainingData[inTrain, ]\r\nmodelTesting &lt;- trainingData[-inTrain, ]\r\ndim(modelTraining)</code></pre>\r\n<pre><code>## [1] 11776    54</code></pre>\r\n<pre class=\"r\"><code>dim(modelTesting)</code></pre>\r\n<pre><code>## [1] 7846   54</code></pre>\r\n</div>\r\n</div>\r\n<div id=\"model-development\" class=\"section level1\">\r\n<h1>Model Development</h1>\r\n<div id=\"selecting-a-model\" class=\"section level2\">\r\n<h2>Selecting a Model</h2>\r\n<p>We can now create models based on the pre-processed data set. In order to avoid overfitting and to reduce out of sample errors, we use TrainControl to perform 5-fold cross validation instead of the default 10. The six models estimated are: Random forest, Support Vector Machine (both radial and linear), a Neural net, a Bayes Generalized linear model and a Logit Boosted model. We will also use the parallel library to maximize multiple cores on the host machine.</p>\r\n<pre class=\"r\"><code>library(parallel); library(doParallel)\r\nregisterDoParallel(clust &lt;- makeForkCluster(detectCores()))\r\n\r\n# Set up the control for the models\r\nmodelCTRL &lt;- trainControl(method = &quot;cv&quot;, number = 5, verboseIter=FALSE , preProcOptions=&quot;pca&quot;, allowParallel=TRUE)\r\n\r\n# Run the six models\r\nrandomForest &lt;- train(classe ~ ., data = modelTraining, method = &quot;rf&quot;, trControl= modelCTRL, ntree=50)\r\nsvmr &lt;- train(classe ~ ., data = modelTraining, method = &quot;svmRadial&quot;, trControl= modelCTRL)\r\nneural &lt;- train(classe ~ ., data = modelTraining, method = &quot;nnet&quot;, trControl= modelCTRL)</code></pre>\r\n<pre><code>## Loading required package: nnet</code></pre>\r\n<pre><code>## # weights:  300\r\n## initial  value 23503.680401 \r\n## iter  10 value 18281.619047\r\n## iter  20 value 17962.866579\r\n## iter  30 value 17154.108235\r\n## iter  40 value 16969.304281\r\n## iter  50 value 16803.429215\r\n## iter  60 value 16650.033797\r\n## iter  70 value 16504.447574\r\n## iter  80 value 16480.448570\r\n## iter  90 value 16375.929941\r\n## iter 100 value 16315.117373\r\n## final  value 16315.117373 \r\n## stopped after 100 iterations</code></pre>\r\n<pre class=\"r\"><code>svml &lt;- train(classe ~ ., data = modelTraining, method = &quot;svmLinear&quot;, trControl= modelCTRL)\r\nbayesglm &lt;- train(classe ~ ., data = modelTraining, method = &quot;bayesglm&quot;, trControl= modelCTRL)\r\nlogitboost &lt;- train(classe ~ ., data = modelTraining, method = &quot;LogitBoost&quot;, trControl= modelCTRL)</code></pre>\r\n<pre><code>## Loading required package: caTools</code></pre>\r\n<pre class=\"r\"><code>stopCluster(clust)</code></pre>\r\n<p>Let’s compare the accuracy of the models. Random Forest, LogitBoost, and SVM (radial) have the best accuracy of all size models.</p>\r\n<pre class=\"r\"><code>model &lt;- c(&quot;Random Forest&quot;,&quot;LogitBoost&quot;,&quot;SVM (radial)&quot;, &quot;SVM (linear)&quot;, &quot;Neural Net&quot;, &quot;Bayes GLM&quot;)\r\nAccuracy &lt;- c(max(randomForest$results$Accuracy),\r\n         max(logitboost$results$Accuracy),\r\n         max(svmr$results$Accuracy),\r\n         max(svml$results$Accuracy),\r\n         max(neural$results$Accuracy),\r\n         max(bayesglm$results$Accuracy))\r\n        \r\nKappa &lt;- c(max(randomForest$results$Kappa),\r\n         max(logitboost$results$Kappa),\r\n         max(svmr$results$Kappa),\r\n         max(svml$results$Kappa),\r\n         max(neural$results$Kappa),\r\n         max(bayesglm$results$Kappa))  \r\n\r\nperformance &lt;- cbind(model, Accuracy, Kappa)\r\nknitr::kable(performance)</code></pre>\r\n<table>\r\n<thead>\r\n<tr class=\"header\">\r\n<th align=\"left\">model</th>\r\n<th align=\"left\">Accuracy</th>\r\n<th align=\"left\">Kappa</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr class=\"odd\">\r\n<td align=\"left\">Random Forest</td>\r\n<td align=\"left\">0.994735472091063</td>\r\n<td align=\"left\">0.993340443450489</td>\r\n</tr>\r\n<tr class=\"even\">\r\n<td align=\"left\">LogitBoost</td>\r\n<td align=\"left\">0.92972348753086</td>\r\n<td align=\"left\">0.910549154246006</td>\r\n</tr>\r\n<tr class=\"odd\">\r\n<td align=\"left\">SVM (radial)</td>\r\n<td align=\"left\">0.917714570664534</td>\r\n<td align=\"left\">0.895782596238381</td>\r\n</tr>\r\n<tr class=\"even\">\r\n<td align=\"left\">SVM (linear)</td>\r\n<td align=\"left\">0.787109308208236</td>\r\n<td align=\"left\">0.729396971598701</td>\r\n</tr>\r\n<tr class=\"odd\">\r\n<td align=\"left\">Neural Net</td>\r\n<td align=\"left\">0.431975847961046</td>\r\n<td align=\"left\">0.280604261183444</td>\r\n</tr>\r\n<tr class=\"even\">\r\n<td align=\"left\">Bayes GLM</td>\r\n<td align=\"left\">0.401239518617301</td>\r\n<td align=\"left\">0.234750696831903</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n</div>\r\n<div id=\"cross-validation-and-out-of-sample-error\" class=\"section level2\">\r\n<h2>Cross Validation and Out of Sample Error</h2>\r\n<p>We now predict new values within the test set that we created for random forest, LogitBoost, and SVM(radial) models.</p>\r\n<pre class=\"r\"><code>randomForestPrediction &lt;- predict(randomForest, modelTesting)\r\nlogitboostPrediction &lt;- predict(logitboost, modelTesting)\r\nsvmrPrediction &lt;- predict(svmr, modelTesting)</code></pre>\r\n<p>Let’s check to see if the models give same predictions. Random Forest and LogitBoost provide the greatest number of matches.</p>\r\n<pre class=\"r\"><code>firstPrediction &lt;- data.frame(cbind(randomForestPrediction, logitboostPrediction))\r\nfirstPrediction$same &lt;- with(firstPrediction, randomForestPrediction == logitboostPrediction)\r\ncolnames(firstPrediction) &lt;- c(&quot;Random Forest&quot;, &quot;LogitBoost&quot;, &quot;SamePrediction&quot;)\r\n\r\n# Number of matches between Random Forest and LogitBoost\r\ndim(firstPrediction[firstPrediction$SamePrediction==TRUE,])</code></pre>\r\n<pre><code>## [1] 7407    3</code></pre>\r\n<pre class=\"r\"><code>secondPrediction &lt;- data.frame(cbind(randomForestPrediction, svmrPrediction))\r\nsecondPrediction$same &lt;- with(secondPrediction, randomForestPrediction == svmrPrediction)\r\ncolnames(secondPrediction) &lt;- c(&quot;Random Forest&quot;, &quot;SVM (radial)&quot;, &quot;SamePrediction&quot;)\r\n\r\n# Number of matches between Random Forest and SVM (radial)\r\ndim(secondPrediction[secondPrediction$SamePrediction==TRUE,])</code></pre>\r\n<pre><code>## [1] 7306    3</code></pre>\r\n<p>We can calculate the expected out of sample error based on the test set that we created for cross-validation – using the Random Forest model. The accuracy of the Random Forest model is 99.4735472%. So I expect the out of sample error estimate to be less than 41.305486, where 7,846 is the number of rows in the modelTesting dataset. The prediction results with random forest are encouraging in looking at the confusion matrix. The confusionMatrix shows that 24 predictions are inaccurate, which is better than my expected out of sample error estimate.</p>\r\n<pre class=\"r\"><code>myMatrix &lt;- confusionMatrix(randomForestPrediction, modelTesting[, &quot;classe&quot;])\r\nmyMatrix</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 2231    8    0    0    0\r\n##          B    0 1508    5    0    0\r\n##          C    0    2 1363    7    0\r\n##          D    0    0    0 1278    1\r\n##          E    1    0    0    1 1441\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9968          \r\n##                  95% CI : (0.9953, 0.9979)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.996           \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9996   0.9934   0.9963   0.9938   0.9993\r\n## Specificity            0.9986   0.9992   0.9986   0.9998   0.9997\r\n## Pos Pred Value         0.9964   0.9967   0.9934   0.9992   0.9986\r\n## Neg Pred Value         0.9998   0.9984   0.9992   0.9988   0.9998\r\n## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2843   0.1922   0.1737   0.1629   0.1837\r\n## Detection Prevalence   0.2854   0.1928   0.1749   0.1630   0.1839\r\n## Balanced Accuracy      0.9991   0.9963   0.9975   0.9968   0.9995</code></pre>\r\n</div>\r\n</div>\r\n<div id=\"prediction-submission\" class=\"section level1\">\r\n<h1>Prediction Submission</h1>\r\n<p>Generate the files for submission using the final test data provided.</p>\r\n<pre class=\"r\"><code>answers &lt;- predict(randomForest, finalTestingData)\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(&quot;problem_id_&quot;,i,&quot;.txt&quot;)\r\n    write.table(x[i],file=filename,quote=FALSE, row.names=FALSE, col.names=FALSE)\r\n  }\r\n}\r\npml_write_files(answers)</code></pre>\r\n</div>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}