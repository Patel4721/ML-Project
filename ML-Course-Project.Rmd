---
title: "Machine Learning Course Project"
author: "Shailesh Patel"
date: "March 22, 2015"
output:
  html_document:
    keep_md: yes
---

#Background

This project is related to Human Activity Research (HAR) and the use of personal activity data captured via devices such as Jawbone Up, Nike FuelBand, and Fitbit.  The dataset contains data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions. We are provided a data set from:

http://groupware.les.inf.puc-rio.br/har

Our goal is to use the data set to predict the quality of the exercise the user conducted.  The quality outcome is defined as classes as follows:

A - exactly according to the specification
B - throwing the elbows to the front
C - lifting the dumbbell only halfway
D - lowering the dumbbell only halfway
E - throwing the hips to the front

#Reproducibility
We will first load the necessary libraries and set the seed.
```{r Reproducibility, warning=FALSE}
# Install and load the libraries we will need - only if needed
list.of.packages <- c("caret", 
                      "kernlab", 
                      "randomForest", 
                      "Hmisc", 
                      "abind", 
                      "arm", 
                      "rpart", 
                      "doParallel")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(caret)
library(kernlab)
library(randomForest)
library(Hmisc)
library(abind)
library(arm)
library(rpart)
library(doParallel)

set.seed(8666)
```

#Data 
##Load Libraries and Download Data
The training data for this project wre loaded from the following URLs:

Training data:  https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Download the data for training and for final testing. We call the testing data "final testing" data to avoid confusion with the test set we'll create to validate our model.  Final testing is used for the submission of the results of this work. 

```{r Load Data, , warning=FALSE}
trainingcsv <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingcsv <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(trainingcsv, "pml-training.csv", method="curl")
download.file(testingcsv, "pml-testing.csv", method="curl")

date() # Log the date and time when the file is downloaded

trainingData <- read.csv("pml-training.csv")
finalTestingData <- read.csv("pml-testing.csv")
```

##Cleaning Data
Make sure all the column names between the training and final testing data sets are identical. We know the final testing data does not contain the "classe" column.   

```{r Clean Data - Check Columns, warning=FALSE}
all.equal(colnames(finalTestingData)[1:length(colnames(finalTestingData))-1], colnames(trainingData)[1:length(colnames(trainingData))-1])
```
To clean the data, we are first removing the near zero variance predictors. Many variables have a high degree of correlation. 
```{r Clean Data - Remove Near Zero Variance}
nzvValues <- nearZeroVar(trainingData)
trainingData <- trainingData[, -nzvValues]
dim(trainingData)
```
We then remove the first five predictors/columns of the training data id, user names, and time stamps are not very useful with models.  
```{r Clean Data - Remove Unnecessary Columns, warning=FALSE}
trainingData <- trainingData[, -(1:5)]
dim(trainingData)
```
Lastly, we remove columns with too many NA's. We want to reduce predictors with more than 60% missing data.  
```{r Clean Data - Remove NAs, warning=FALSE}
remove <- sapply(colnames(trainingData), function(x) if(sum(is.na(trainingData[, x])) 
                                                        > 0.60*nrow(trainingData))
  {return(TRUE)
} else {
return(FALSE)})
trainingData <- trainingData[, !remove]
dim(trainingData)
```
After cleaning the data the following predictors are included in the models.
```{r Clean Data - Resulting Predictors, warning=FALSE}
names(trainingData)
```

##Partitioning Data
We'll first partition the data so that 60% of the data will be used for training and 40% will be used for testing the models. 
```{r Partition Data, warning=FALSE}
inTrain <- createDataPartition(y=trainingData$classe, p=0.6, list=FALSE)
modelTraining <- trainingData[inTrain, ]
modelTesting <- trainingData[-inTrain, ]
dim(modelTraining)
dim(modelTesting)
```

#Model Development
##Selecting a Model
We can now create models based on the pre-processed data set. In order to avoid overfitting and to reduce out of sample errors, we use TrainControl to perform 5-fold cross validation instead of the default 10. The six models estimated are: Random forest, Support Vector Machine (both radial and linear), a Neural net, a Bayes Generalized linear model and a Logit Boosted model. We will also use the parallel library to maximize multiple cores on the host machine.  

```{r Model Selection, warning=FALSE}
library(parallel); library(doParallel)
registerDoParallel(clust <- makeForkCluster(detectCores()))

# Set up the control for the models
modelCTRL <- trainControl(method = "cv", number = 5, verboseIter=FALSE , preProcOptions="pca", allowParallel=TRUE)

# Run the six models
randomForest <- train(classe ~ ., data = modelTraining, method = "rf", trControl= modelCTRL, ntree=50)
svmr <- train(classe ~ ., data = modelTraining, method = "svmRadial", trControl= modelCTRL)
neural <- train(classe ~ ., data = modelTraining, method = "nnet", trControl= modelCTRL)
svml <- train(classe ~ ., data = modelTraining, method = "svmLinear", trControl= modelCTRL)
bayesglm <- train(classe ~ ., data = modelTraining, method = "bayesglm", trControl= modelCTRL)
logitboost <- train(classe ~ ., data = modelTraining, method = "LogitBoost", trControl= modelCTRL)

stopCluster(clust)
```

Let's compare the accuracy of the models. Random Forest, LogitBoost, and SVM (radial) have the best accuracy of all size models.  

```{r Model Comparison, warning=FALSE}
model <- c("Random Forest","LogitBoost","SVM (radial)", "SVM (linear)", "Neural Net", "Bayes GLM")
Accuracy <- c(max(randomForest$results$Accuracy),
         max(logitboost$results$Accuracy),
         max(svmr$results$Accuracy),
         max(svml$results$Accuracy),
         max(neural$results$Accuracy),
         max(bayesglm$results$Accuracy))
        
Kappa <- c(max(randomForest$results$Kappa),
         max(logitboost$results$Kappa),
         max(svmr$results$Kappa),
         max(svml$results$Kappa),
         max(neural$results$Kappa),
         max(bayesglm$results$Kappa))  

performance <- cbind(model, Accuracy, Kappa)
knitr::kable(performance)
```

##Cross Validation and Out of Sample Error
We now predict new values within the test set that we created for random forest, LogitBoost, and SVM(radial) models.  

```{r Cross Validation, warning=FALSE}
randomForestPrediction <- predict(randomForest, modelTesting)
logitboostPrediction <- predict(logitboost, modelTesting)
svmrPrediction <- predict(svmr, modelTesting)
```

Let's check to see if the models give same predictions. Random Forest and LogitBoost provide the greatest number of matches.  

```{r Cross Validation Prediction Comparison, warning=FALSE}
firstPrediction <- data.frame(cbind(randomForestPrediction, logitboostPrediction))
firstPrediction$same <- with(firstPrediction, randomForestPrediction == logitboostPrediction)
colnames(firstPrediction) <- c("Random Forest", "LogitBoost", "SamePrediction")

# Number of matches between Random Forest and LogitBoost
dim(firstPrediction[firstPrediction$SamePrediction==TRUE,])

secondPrediction <- data.frame(cbind(randomForestPrediction, svmrPrediction))
secondPrediction$same <- with(secondPrediction, randomForestPrediction == svmrPrediction)
colnames(secondPrediction) <- c("Random Forest", "SVM (radial)", "SamePrediction")

# Number of matches between Random Forest and SVM (radial)
dim(secondPrediction[secondPrediction$SamePrediction==TRUE,])
```

We can calculate the expected out of sample error based on the test set that we created for cross-validation -- using the Random Forest model.  The accuracy of the Random Forest model is `r max(randomForest$results$Accuracy)*100`%. So I expect the out of sample error estimate to be less than `r 7846 - 7846*max(randomForest$results$Accuracy)`, where 7,846 is the number of rows in the modelTesting dataset. The prediction results with random forest are encouraging in looking at the confusion matrix.  The confusionMatrix shows that 24 predictions are inaccurate, which is better than my expected out of sample error estimate.  

```{r Cross Validation Confusion Matrix, warning=FALSE}
myMatrix <- confusionMatrix(randomForestPrediction, modelTesting[, "classe"])
myMatrix
```

#Prediction Submission
Generate the files for submission using the final test data provided.
```{r Prediction Submission, warning=FALSE}
answers <- predict(randomForest, finalTestingData)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}
pml_write_files(answers)
```


