---
title: "Machine Learning Course Project"
author: "Shailesh Patel"
date: "March 22, 2015"
output:
  html_document:
    keep_md: yes
---

#Background

This project is related to Human Activity Research (HAR) and the use of personal activity data captured via devices such as Jawbone Up, Nike FuelBand, and Fitbit.  The dataset contains data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions. We are provided a data set from:

http://groupware.les.inf.puc-rio.br/har

Our goal is to use the data set to predict the quality of the exercise the user conducted.  The quality outcome is defined as classes as follows:

A - exactly according to the specification
B - throwing the elbows to the front
C - lifting the dumbbell only halfway
D - lowering the dumbbell only halfway
E - throwing the hips to the front

#Reproducibility
We will first load the necessary libraries and set the seed.
```{r Reproducibility, warning=FALSE}
# Install and load the libraries we will need - only if needed
list.of.packages <- c("caret", 
                      "kernlab", 
                      "randomForest", 
                      "Hmisc", 
                      "abind", 
                      "arm", 
                      "rpart", 
                      "parallel",
                      "doParallel",
                      "gbm",
                      "plyr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(caret)
library(kernlab)
library(randomForest)
library(Hmisc)
library(abind)
library(arm)
library(rpart)
library(parallel)
library(doParallel)
library(gbm)
library(plyr)

set.seed(8666)
```

#Data 
##Load Libraries and Download Data
The training data for this project wre loaded from the following URLs:

Training data:  https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Download the data for training and for final testing. We call the testing data "final testing" data to avoid confusion with the test set we'll create to validate our model.  Final testing is used for the submission of the results of this work. 

```{r Load Data, , warning=FALSE}
trainingcsv <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingcsv <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(trainingcsv, "pml-training.csv", method="curl")
download.file(testingcsv, "pml-testing.csv", method="curl")

date() # Log the date and time when the file is downloaded

trainingData <- read.csv("pml-training.csv")
finalTestingData <- read.csv("pml-testing.csv")
```

##Cleaning Data
Make sure all the column names between the training and final testing data sets are identical. We know the final testing data does not contain the "classe" column.   

```{r Clean Data - Check Columns, warning=FALSE}
all.equal(colnames(finalTestingData)[1:length(colnames(finalTestingData))-1], colnames(trainingData)[1:length(colnames(trainingData))-1])
```
To clean the data, we are first removing the near zero variance predictors. Many variables have a high degree of correlation. 
```{r Clean Data - Remove Near Zero Variance}
nzvValues <- nearZeroVar(trainingData)
trainingData <- trainingData[, -nzvValues]
dim(trainingData)
```
We then remove the first five predictors/columns of the training data id, user names, and time stamps are not very useful with models.  
```{r Clean Data - Remove Unnecessary Columns, warning=FALSE}
trainingData <- trainingData[, -(1:5)]
dim(trainingData)
```
Lastly, we remove columns with too many NA's. We want to reduce predictors with more than 60% missing data.  
```{r Clean Data - Remove NAs, warning=FALSE}
remove <- sapply(colnames(trainingData), function(x) if(sum(is.na(trainingData[, x])) 
                                                        > 0.60*nrow(trainingData))
  {return(TRUE)
} else {
return(FALSE)})
trainingData <- trainingData[, !remove]
dim(trainingData)
```
After cleaning the data the following predictors are included in the models.
```{r Clean Data - Resulting Predictors, warning=FALSE}
names(trainingData)
```

##Partitioning Data
We'll first partition the data so that 60% of the data will be used for training and 40% will be used for testing the models. 
```{r Partition Data, warning=FALSE}
inTrain <- createDataPartition(y=trainingData$classe, p=0.6, list=FALSE)
modelTraining <- trainingData[inTrain, ]
modelTesting <- trainingData[-inTrain, ]
dim(modelTraining)
dim(modelTesting)
```

#Model Development
##Selecting a Model
We can now create models based on the pre-processed data set. In order to avoid overfitting and to reduce out of sample errors, we use TrainControl to perform 5-fold cross validation instead of the default 10. The seven models estimated are: Random forest, LogitBoost, Tree Boost, Support Vector Machine (both radial and linear), a Neural net, a Bayes Generalized linear model. We will also use the parallel library to maximize multiple cores on the host machine.  

```{r Model Selection, warning=FALSE}
registerDoParallel(clust <- makeForkCluster(detectCores()))

# Set up the control for the models
modelCTRL <- trainControl(method = "cv", number = 5, verboseIter=FALSE , preProcOptions="pca", allowParallel=TRUE)

# Run the seven models
randomForest <- train(classe ~ ., data = modelTraining, method = "rf", trControl= modelCTRL, ntree=50)
svmr <- train(classe ~ ., data = modelTraining, method = "svmRadial", trControl= modelCTRL)
neural <- train(classe ~ ., data = modelTraining, method = "nnet", trControl= modelCTRL)
svml <- train(classe ~ ., data = modelTraining, method = "svmLinear", trControl= modelCTRL)
bayesglm <- train(classe ~ ., data = modelTraining, method = "bayesglm", trControl= modelCTRL)
logitboost <- train(classe ~ ., data = modelTraining, method = "LogitBoost", trControl= modelCTRL)
treeboost <- train(classe ~ ., data = modelTraining, method = "gbm", trControl= modelCTRL, verbose=FALSE)

stopCluster(clust)
```

Let's compare the accuracy of the models. As show in the table below, Random Forest, Tree Boost, and LogitBoost have the best accuracy of all size models.  

```{r Model Comparison, warning=FALSE}
model <- c("Random Forest","LogitBoost","SVM (radial)", "SVM (linear)", "Neural Net", "Bayes GLM", "Tree Boost")
Accuracy <- c(max(randomForest$results$Accuracy),
         max(logitboost$results$Accuracy),
         max(svmr$results$Accuracy),
         max(svml$results$Accuracy),
         max(neural$results$Accuracy),
         max(bayesglm$results$Accuracy),
         max(treeboost$results$Accuracy))
        
Kappa <- c(max(randomForest$results$Kappa),
         max(logitboost$results$Kappa),
         max(svmr$results$Kappa),
         max(svml$results$Kappa),
         max(neural$results$Kappa),
         max(bayesglm$results$Kappa),
         max(treeboost$results$Kappa))  

performance <- cbind(model, Accuracy, Kappa)
knitr::kable(performance[order(-Accuracy), ], caption="Model Comparison")
```

##Cross Validation and Out of Sample Error
We now predict new values within the test set that we created for random forest, LogitBoost, and SVM(radial) models.  

```{r Cross Validation, warning=FALSE}
randomForestPrediction <- predict(randomForest, modelTesting)
logitboostPrediction <- predict(logitboost, modelTesting)
svmrPrediction <- predict(svmr, modelTesting)
treeboostPrediction <- predict(treeboost, modelTesting)
```

Let's check to see if the models give same predictions. Tree Boost and LogitBoost provide the greatest number of matches with Random Forest.

```{r Cross Validation Prediction Comparison, warning=FALSE}
firstPrediction <- data.frame(cbind(randomForestPrediction, logitboostPrediction))
firstPrediction$same <- with(firstPrediction, randomForestPrediction == logitboostPrediction)
colnames(firstPrediction) <- c("Random Forest", "LogitBoost", "SamePrediction")

secondPrediction <- data.frame(cbind(randomForestPrediction, treeboostPrediction))
secondPrediction$same <- with(secondPrediction, randomForestPrediction == treeboostPrediction)
colnames(secondPrediction) <- c("Random Forest", "Tree Boost", "SamePrediction")

firstMatch <- nrow(firstPrediction[firstPrediction$SamePrediction==TRUE,])
secondMatch <- nrow(secondPrediction[secondPrediction$SamePrediction==TRUE,])

matchResults <- data.frame(firstMatch, secondMatch)
colnames(matchResults) <- c("LogitBoost", "Tree Boost")
knitr::kable(matchResults, caption="Number of Matches with Random Forest")
```

We can calculate the expected out of sample error based on the test set that we created for cross-validation -- using the Random Forest model.  

```{r Cross Validation Confusion Matrix, warning=FALSE}
# Create matrix with 1's in diagonal so we can use that to get only the diagonal
# in the confustionMatrix.  This is needed to calculate the number of observations
# that are errors.

myMatrix <- confusionMatrix(randomForestPrediction, modelTesting[, "classe"])
n <- matrix(c(1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1), nrow=5, ncol=5)
nonerror <- sum(myMatrix$table * n) # the number of observations without errors
totalobservations <- sum(myMatrix$table) # the number of total observations
myMatrix # show the confustionMatrix

```
The accuracy of the Random Forest model is `r max(randomForest$results$Accuracy)*100`%. So I expect the out of sample error estimate to be less than `r 7846 - 7846*max(randomForest$results$Accuracy)` out of 7,846 observations in the modelTesting dataset. The prediction results with random forest are encouraging in looking at the confusion matrix.  The confusionMatrix shows that `r totalobservations - nonerror` predictions are inaccurate, which is better than my expected out of sample error estimate.  

#Prediction Submission
Generate the files for submission using the final test data provided.
```{r Prediction Submission, warning=FALSE}
answers <- predict(randomForest, finalTestingData)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}
pml_write_files(answers)
```


